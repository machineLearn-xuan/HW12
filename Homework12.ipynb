{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework12\n",
    "\n",
    "Exercises with Neural Networks\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Get familiar with neural network setup, design, data preparation and training process\n",
    "- Practice setting up the ingredients and parameters for the training loop\n",
    "- Experiment with neural networks for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/nn_utils.py\n",
    "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from data_utils import LFWUtils\n",
    "\n",
    "from nn_utils import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Classification\n",
    "\n",
    "We saw a neural network that performs linear regression in the [WK12](https://github.com/PSAM-5020-2025S-A/WK12) notebook, but, how can we use one for classification ?\n",
    "\n",
    "The modifications aren't too hard. We first need to have as many output nodes as we have classes in our dataset. If we're trying to classify images into $25$ categories, we need $25$ output neurons.\n",
    "\n",
    "We also need to use a different kind of loss/cost function that will force the network to only have one activated neuron in its output layer, per input. If we give it data for something that should be classified as class $7$, the neuron that represents class $7$ at the output layer should be activated way more than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "The cost of being wrong for a classification network is calculated using a function called *Cross Entropy*. It basically measures how different two distributions are, and in this case, we want to compare the distributions of values from the last layer of our network and what we expected to get at the last layer, over all of the inputs that it was shown.\n",
    "\n",
    "For example, let's say we are training a classification neural network on a dataset with $3$ classes. The correct distribution of labels in the training dataset is:\n",
    "\n",
    "| $\\text{class}$ | $\\text{distribution}$ |\n",
    "|----|----|\n",
    "| $0$ | $45\\%$ |\n",
    "| $1$ | $35\\%$ |\n",
    "| $2$ | $20\\%$ |\n",
    "\n",
    "but instead we get the following distribution for the predictions:\n",
    "\n",
    "| $\\text{class}$ | $\\text{distribution}$ |\n",
    "|----|----|\n",
    "| $0$ | $32\\%$ |\n",
    "| $1$ | $32\\%$ |\n",
    "| $2$ | $36\\%$ |\n",
    "\n",
    "*Cross Entropy* is a way to quantify how different these two distributions are and, hopefully, guide our network parameters to make them more similar.\n",
    "\n",
    "The actual cost calculation is a sum of how different each prediction is from a \"perfect\" prediction. For example, if we're trying to classify something as class $0$, we should get output neuron activations that are something like: `[1.0, 0.0, 0.0]`. If instead we get: `[0.5, 0.8, 0.1]`, the *Cross Entropy* contribution for this sample is calculated as:\n",
    "$$\\displaystyle - \\log\\left(\\frac{e^{0.5}}{e^{0.5} + e^{0.8} + e^{0.1}}\\right)$$\n",
    "\n",
    "By using this as the cost function, the neural network can optimize its parameters to increase the value of the first output, while decreasing the values of the other outputs. It doesn't have to get `[1.0, 0.0, 0.0]` exactly, so the network optimizes its parameters to make the first output as different as possible from the others.\n",
    "\n",
    "Instead of writing our own cost function like we did with the euclidean cost for linear regression, we'll use the built in `CrossEntropyLoss` function in the `PyTorch` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process ...\n",
    "\n",
    "... is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with the Data\n",
    "\n",
    "As always, we start with the data, and this time put the pixel and label information straight into `Tensor` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"])\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(test[\"pixels\"])\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train !\n",
    "\n",
    "Let's create a single layer neural network, like the one from class, and train it with the training data.\n",
    "\n",
    "In addition to the actual model/network, we also need an optimizer and a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create the model and optimizer, the loss function is already defined\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loop\n",
    "\n",
    "Create a training loop like we saw in class.\n",
    "\n",
    "This loop should:\n",
    "- Predict classes by feeding all of the inputs into the `model`\n",
    "- Measure `loss` (this is just `loss_fn(predicted, actual)`)\n",
    "- Get the optimizer to compute gradients\n",
    "- Update parameters\n",
    "\n",
    "The loop should be repeated as long as the loss keeps improving, and it doesn't look like the model is overfitting with the training data.\n",
    "\n",
    "In order to check if the model is overfitting, we can sporadically run evaluations within the training loop in order to see if the model performs similarly with `train` and `test` data.\n",
    "\n",
    "But ! Our network actually outputs a series of values for each image that we give it. In order to determine the exact class number of its predictions, we have to find the index of the output neuron with the largest value, which is an operation called `argmax()` (similar to `argsort()` from week 10).\n",
    "\n",
    "It's not hard to do this manually, but we can use the `get_labels(model, inputs)` function inside the `nn_utils` file to run our `model` on all of the data in a given dataset and return the predicted labels for all of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: iterate epochs\n",
    "  # TODO: iterate batches\n",
    "    # TODO: predict\n",
    "    # TODO: measure loss\n",
    "    # TODO: compute gradient and step optimizer\n",
    "    # TODO: show progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "This should be similar to the last error values seen during training, but sometimes it changes a bit...\n",
    "\n",
    "Not a bad idea to check the accuracy of the model using the `classification_error()` function, and look at some confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: classification error for train and test data\n",
    "# TODO: confusion matrices for train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and PCA\n",
    "\n",
    "We are seeing how Neural Networks can be _easy_ to build and explain in generic/abstract terms (a bunch of little operators that perform weighted sums of their inputs), but in reality can be really difficult and opaque to steer.\n",
    "\n",
    "In theory, a couple of well placed neuron layers, with the right hyperparameters, learning rate, loss function, architecture and a good amount of data, can learn to extract information like polynomial features, clusters or even PCA components. But... that's not always the case and sometimes it's not a bad idea to push/bias/encourage the network to go down a certain path.\n",
    "\n",
    "One way to do this is to pre-process our inputs and do a bit of feature extraction ourselves.\n",
    "\n",
    "Let's see if we can improve this face classification network by using PCA information instead of pixel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PCA\n",
    "\n",
    "We're going to repeat the training, but this time our data is going to be scaled and PCA'd before going into the neural network.\n",
    "\n",
    "So, the data preparation flow should be:\n",
    "- Scale data for `PCA`\n",
    "- Perform `PCA`\n",
    "\n",
    "We need one `StandardScaler()` object and one `PCA()` object.\n",
    "\n",
    "The `train` data goes through the `fit_transform()` function of these objects, while the `test` data only goes through `transform()`.\n",
    "\n",
    "For the `PCA`, we can aim for an explained variance of $99\\%$. This should reduce the number of features significantly to allow us to experiment with our network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Scale\n",
    "# TODO: PCA\n",
    "# TODO: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "Re-create model, optimizer, loss function, then re-run the training loop and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Model, Optimizer and Loss Function\n",
    "# TODO: Training loop\n",
    "# TODO: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "So... What happens ?<br>\n",
    "How does training on the <code>PCA</code> data compare to training on the regular data ?\n",
    "\n",
    "What else does <code>PCA</code> afford us in this case ? ...<br>\n",
    "How does adding extra layers in the original network compare to adding extra layers in the <code>PCA</code> network?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
